# -*- coding: utf-8 -*-
"""Heart Failure Prediction .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iIQQ8sdymHyuzDx8HT4R1YLk01OOjsGu
"""

#loading dataset
import numpy as np
import pandas as pd
#visualisation
import matplotlib
import matplotlib.pyplot as plt
#EDA
from collections import Counter
import seaborn as sns
# data splitting
from sklearn.model_selection import train_test_split
# data modeling
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
#ensembling
from mlxtend.classifier import StackingCVClassifier
#ingore the warning
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('/content/heart.3.csv')
data.head()

# print last 5 rows of the data
data.tail()

# number of rows and columns in the data
data.shape

data.info()

# checking for missing values
data.isnull().sum()

""" understand our columns better:"""

info = ["age","1: male, 0: female","chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic","resting blood pressure"," serum cholestoral in mg/dl","fasting blood sugar > 120 mg/dl","resting electrocardiographic results (values 0,1,2)"," maximum heart rate achieved","exercise induced angina","oldpeak = ST depression induced by exercise relative to rest","the slope of the peak exercise ST segment","number of major vessels (0-3) colored by flourosopy","thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"]



for i in range(len(info)):
    print(data.columns[i]+":\t\t\t"+info[i])

# transform data to numeric to enable further analysis
data = data.apply(pd.to_numeric)
data.dtypes

"""Checking correlation between columns"""

print(data.corr()["target"].abs().sort_values(ascending=False))

"""This shows that most columns are moderately correlated with target, but 'fbs' is very weakly correlated."""

# statistical measures about the data
data.describe()

"""Exploratory Data Analysis (EDA)"""

#get correlations of each features in dataset
corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(12,12))
#plot heat map
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="winter")

# plot histograms for each variable
data.hist(figsize = (12, 12))
plt.show()

pd.crosstab(data.age,data.target).plot(kind="bar",figsize=(20,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# checking the distribution of Target Variable
data['target'].value_counts()

y = data["target"]
X = data.drop('target',axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 2)

"""Before applying algorithm we should check whether the data is equally splitted or not, because if data is not splitted equally it will cause for data imbalacing problem"""

print(y_test.unique())
Counter(y_train)

m1 = 'Random Forest Classfier'
rf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)
rf.fit(X_train,y_train)
rf_predicted = rf.predict(X_test)
rf_conf_matrix = confusion_matrix(y_test, rf_predicted)
rf_acc_score = accuracy_score(y_test, rf_predicted)
cm=confusion_matrix(y_test,rf_predicted)
sns.heatmap(cm, annot=True,cmap='winter',linewidths=0.3, linecolor='black',annot_kws={"size": 20})
TP=cm[0][0]
TN=cm[1][1]
FN=cm[1][0]
FP=cm[0][1]
print('\n')
print("Accuracy of Random Forest:",rf_acc_score*100,'\n')
print('Testing Accuracy for Random Forest:',(TP+TN)/(TP+TN+FN+FP))
print('Testing Sensitivity for Random Forest:',(TP/(TP+FN)))
print('Testing Specificity for Random Forest:',(TN/(TN+FP)))
print('Testing Precision for Random Forest:',(TP/(TP+FP)))
print('\n')
print(classification_report(y_test,rf_predicted))

m2 = 'DecisionTreeClassifier'
dt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)
dt.fit(X_train, y_train)
dt_predicted = dt.predict(X_test)
dt_conf_matrix = confusion_matrix(y_test, dt_predicted)
dt_acc_score = accuracy_score(y_test, dt_predicted)
cm= confusion_matrix(y_test,dt_predicted)
sns.heatmap(cm, annot=True,cmap='winter',linewidths=0.3, linecolor='black',annot_kws={"size": 20})
TP=cm[0][0]
TN=cm[1][1]
FN=cm[1][0]
FP=cm[0][1]
print('\n')
print("Accuracy of DecisionTreeClassifier:",dt_acc_score*100,'\n')
print('Testing Accuracy for Decision Tree:',(TP+TN)/(TP+TN+FN+FP))
print('Testing Sensitivity for Decision Tree:',(TP/(TP+FN)))
print('Testing Specificity for Decision Tree:',(TN/(TN+FP)))
print('Testing Precision for Decision Tree:',(TP/(TP+FP)))
print('\n')
print(classification_report(y_test, dt_predicted))

m3 = 'Support Vector Classifier'
svc =  SVC(kernel='rbf', C=2)
svc.fit(X_train, y_train)
svc_predicted = svc.predict(X_test)
svc_conf_matrix = confusion_matrix(y_test, svc_predicted)
svc_acc_score = accuracy_score(y_test, svc_predicted)
cm= confusion_matrix(y_test,svc_predicted)
sns.heatmap(cm, annot=True,cmap='winter',linewidths=0.3, linecolor='black',annot_kws={"size": 20})
TP=cm[0][0]
TN=cm[1][1]
FN=cm[1][0]
FP=cm[0][1]
print('\n')
print("Accuracy of Support Vector Classifier:",svc_acc_score*100,'\n')
print('Testing Accuracy for SVM:',(TP+TN)/(TP+TN+FN+FP))
print('Testing Sensitivity for Support Vector Classifier:',(TP/(TP+FN)))
print('Testing Specificity for Support Vector Classifier:',(TN/(TN+FP)))
print('Testing Precision for Support Vector Classifier:',(TP/(TP+FP)))
print('\n')
print(classification_report(y_test,svc_predicted))

m4 = 'Logistic Regression'
lr = LogisticRegression()
model = lr.fit(X_train, y_train)
lr_predict = lr.predict(X_test)
lr_conf_matrix = confusion_matrix(y_test, lr_predict)
lr_acc_score = accuracy_score(y_test, lr_predict)
cm= confusion_matrix(y_test,lr_predict)
sns.heatmap(cm, annot=True,cmap='winter',linewidths=0.3, linecolor='black',annot_kws={"size": 20})
TP=cm[0][0]
TN=cm[1][1]
FN=cm[1][0]
FP=cm[0][1]
print('\n')
print("Accuracy of Logistic Regression:",lr_acc_score*100,'\n')
print('Testing Accuracy for Logistic Regression:',(TP+TN)/(TP+TN+FN+FP))
print('Testing Sensitivity for Logistic Regression:',(TP/(TP+FN)))
print('Testing Specificity for Logistic Regression:',(TN/(TN+FP)))
print('Testing Precision for Logistic Regression:',(TP/(TP+FP)))
print('\n')
print(classification_report(y_test, lr_predict))

model_ev = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',
                    'Decision Tree','Support Vector Classifier'], 'Accuracy': [lr_acc_score*100,
                   rf_acc_score*100,dt_acc_score*100,svc_acc_score*100]})
model_ev

scores = [lr_acc_score*100,rf_acc_score*100,dt_acc_score*100,svc_acc_score*100]
algorithms = ["Logistic Regression","Random Forest","Decision Tree","Support Vector Machine"]

model_compare = pd.DataFrame(scores, index=algorithms, columns=['accuracy']).T
model_compare.plot.bar()

"""Ensembling

In order to increase the accuracy of the model we use ensembling. Here we use stacking technique.


"""

scv=StackingCVClassifier(classifiers=[rf,lr,dt,svc],meta_classifier= svc,random_state=42)
scv.fit(X_train,y_train)
scv_predicted = scv.predict(X_test)
scv_conf_matrix = confusion_matrix(y_test, scv_predicted)
scv_acc_score = accuracy_score(y_test, scv_predicted)
cm= confusion_matrix(y_test,scv_predicted)
sns.heatmap(cm, annot=True,cmap='winter',linewidths=0.3, linecolor='black',annot_kws={"size": 20})
TP=cm[0][0]
TN=cm[1][1]
FN=cm[1][0]
FP=cm[0][1]
print('\n')
print("Accuracy of StackingCVClassifier:",scv_acc_score*100,'\n')

"""Building a Predictive System"""

input_data = (34,0,1,118,210,0,1,192,0,0.7,2,0,2)

# change the input data to a numpy array
input_data_as_numpy_array= np.asarray(input_data)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = model.predict(input_data_reshaped)
print(prediction)

if (prediction[0]== 0):
  print('The patient seems to be Normal :)')
else:
  print('The patient seems to be have heart disease :(')